{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 40px;\">\n",
    "    Lunar Landing Deep Q learning Project\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;\">\n",
    "    Creating the agent and methods for training\n",
    "</div>\n",
    "\n",
    "Below is the class concerning the agent but also includes important methods such as replay which will sample from the memory and using to fit the neural network. This method will be called inside another one that will also store experiences and select actions accoding to the epsilon greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "\n",
    "    #creates the neural network\n",
    "    def create_model(self):\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        layers=[]\n",
    "        \n",
    "        #appends the layers according to the hyperparamater self.num_layers\n",
    "        for layer in range(self.num_layers):\n",
    "            if self.num_layers == 1:\n",
    "                layers.append(tf.keras.layers.Dense(4, input_shape=(8,), activation=\"linear\"))\n",
    "                break\n",
    "            elif layer==0:\n",
    "                layers.append(tf.keras.layers.Dense(self.num_neurons[layer], input_shape=(8,), activation=\"relu\"))\n",
    "            elif layer==self.num_layers-1:\n",
    "                layers.append(tf.keras.layers.Dense(4, activation=\"linear\"))\n",
    "            else:\n",
    "                layers.append(tf.keras.layers.Dense(self.num_neurons[layer], activation=\"relu\"))\n",
    "\n",
    "        model = tf.keras.Sequential(layers)\n",
    "        model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"accuracy\"])\n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def __init__(self, discount_rate, epsilon_decay, eps, batch_size, learning_rate, num_layers, num_neurons):\n",
    "        self.env = gym.make(\"LunarLander-v2\")\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.num_layers = int(num_layers)\n",
    "        self.num_neurons = [int(n) for n in num_neurons]\n",
    "        self.model = self.create_model()\n",
    "        self.discount_rate = float(discount_rate)\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = float(epsilon_decay)\n",
    "        self.training_start = 1000\n",
    "        self.eps = int(eps)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "\n",
    "    #picks an action using the epsilon greedy policy\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.model.predict(state, verbose=0))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        return int(action)\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def untuple(self, state):\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        return np.array(state).reshape((1,8))\n",
    "    \n",
    "    #samples a batch from the memory and uses it to\n",
    "    #compute the target from the Bellman equation\n",
    "    #in order to train the model\n",
    "    def replay(self):\n",
    "        if len(self.memory)<self.training_start:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "        states = np.zeros((self.batch_size, 8))\n",
    "        next_states = np.zeros((self.batch_size, 8))\n",
    "        rewards, dones, actions = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            states[i]= batch[i][0]\n",
    "            next_states[i]= batch[i][3]\n",
    "            dones.append(batch[i][-1])\n",
    "            rewards.append(batch[i][2])\n",
    "            actions.append(batch[i][1])\n",
    "\n",
    "        target_state = self.model.predict(states, verbose=0)\n",
    "        target_next_state = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target_state[i][int(actions[i])] = rewards[i]\n",
    "            else:\n",
    "                target_state[i][int(actions[i])]= rewards[i]+self.discount_rate*np.max(target_next_state[i])\n",
    "        \n",
    "        self.model.fit(states, target_state, batch_size = self.batch_size, verbose=0)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "    #trains the model as well as appends each experience the memory\n",
    "    #and replays at every episode\n",
    "    def run(self):\n",
    "        training = True\n",
    "        scores=[]\n",
    "        for episode in range(self.eps):\n",
    "            state = self.env.reset()\n",
    "            state = self.untuple(state)\n",
    "            done = False\n",
    "            score = 0\n",
    "            i=0\n",
    "            if not training:\n",
    "                break\n",
    "            while not done:\n",
    "                i+=1\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info, _ = self.env.step(action)\n",
    "                next_state = self.untuple(next_state)\n",
    "                next_state = np.array(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score+=reward\n",
    "\n",
    "                if done or i>=500:\n",
    "                    print(\"episode: {}/{}, score:{}\".format(episode+1, self.eps, score))\n",
    "                    scores.append(score)\n",
    "                    if score>=300:\n",
    "                        training=False\n",
    "                        self.save(\"model.keras\")\n",
    "                    break\n",
    "\n",
    "                self.replay()\n",
    "\n",
    "        print(\"Average score:{}\".format(sum(scores)/len(scores)))\n",
    "\n",
    "        return int(sum(scores)/len(scores))\n",
    "    \n",
    "\n",
    "    def test(self):\n",
    "        env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "        model = tf.keras.models.load_model(\"model.keras\")\n",
    "        for episode in range(10):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            if isinstance(state, tuple):\n",
    "                state = np.array(state[0])\n",
    "            state=state.reshape((1,8))\n",
    "\n",
    "            score=0\n",
    "            while not done:\n",
    "                action = np.argmax(model.predict(state, verbose=0))\n",
    "                next_state, reward, done, info, _ = env.step(action)\n",
    "                if isinstance(next_state, tuple):\n",
    "                    next_state = np.array(next_state[0])\n",
    "                next_state=next_state.reshape((1,8))\n",
    "                state = next_state\n",
    "                score+=reward\n",
    "\n",
    "                if done or (state[0][6]==1 and state[0][7]==1):\n",
    "                    print(\"episode: {}/{}, score:{}\".format(episode+1, 10, score))\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;\">\n",
    "    Hyperparameter Tuning Using Bayesian Optimization\n",
    "</div>\n",
    "\n",
    "The code below uses the bayesian optimization which essentially figures out what is the best combinations of hyperparamters including the neural network architecture and learning rate, that will maximize the average reward in the first 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Please do not run this code as it is extremely long to run and not necessary)\n",
    "\n",
    "#dictionary that sets the bounds for the hyperparameters\n",
    "parameter_bounds = {\n",
    "    'learning_rate':(1e-4, 1e-2),\n",
    "    'num_layers':(1,5),\n",
    "    'num_neuron1':(1,600),\n",
    "    'num_neuron2':(1,600),\n",
    "    'num_neuron3':(1,600),\n",
    "    'num_neuron4':(1,600),\n",
    "    'num_neuron5':(1,600),\n",
    "    'discount_rate':(0.7, 0.99),\n",
    "    'epsilon_decay':(0.01, 0.1),\n",
    "    'batch_size':(8, 512),\n",
    "}\n",
    "\n",
    "#function used that will be used by the optimizer\n",
    "def optimize(learning_rate,num_neuron1,num_neuron2,num_neuron3,num_neuron4,num_neuron5, num_layers, discount_rate, epsilon_decay, batch_size):\n",
    "    num_neurons=[int(num_neuron1),int(num_neuron2),int(num_neuron3),int(num_neuron4),int(num_neuron5)]\n",
    "    learning_rate = float(learning_rate)\n",
    "    num_layers = int(num_layers)\n",
    "    discount_rate = float(discount_rate)\n",
    "    epsilon_decay = float(epsilon_decay)\n",
    "    batch_size = int(batch_size)\n",
    "    \n",
    "    space_ship = agent(discount_rate, epsilon_decay, 100, batch_size, learning_rate, num_layers, num_neurons)\n",
    "    return space_ship.run()\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize,\n",
    "    pbounds=parameter_bounds\n",
    ")\n",
    "\n",
    "optimizer.maximize()\n",
    "\n",
    "print(optimizer.max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;\">\n",
    "    Training the Agent\n",
    "</div>\n",
    "\n",
    "Using the hyperparameters from the Bayesian Optimization, we will train the agent in a series of 200 epsiodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_ship=agent(batch_size=179, discount_rate=0.99, epsilon_decay=0.995, learning_rate=0.001346, num_layers=3, num_neurons=[128, 128, 0, 0, 0], eps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Please do not run this code as it is extremely long to run and not necessary)\n",
    "space_ship.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;\">\n",
    "    Testing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_ship.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
